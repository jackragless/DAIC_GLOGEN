===GLOGEN GLOSSARY===

|1| paradigm : An example serving as the model for such a pattern.
|2| accelerate : To quicken the natural or ordinary progression or process of.
|3| simpler : comparative form of simple: more simple, less complicated or challenging.
|4| intentionally : In an intentional manner; on purpose.
|5| Bayesian optimization : Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms.
|6| Bayesian : Of or pertaining to Thomas Bayes, English mathematician.
|7| dependencies : plural of dependency. An external component whose functionality is relied on.
|8| optimized : simple past tense and past participle of optimize. To become optimal.
|9| generalization : Inductive reasoning from detailed facts to general principles.
|10| multi-tasking : Alternative form of multitasking.
|11| geared : simple past tense and past participle of gear. To be in, or come into, gear.

===DOCUMENT BODY===

Multi-task optimization is a paradigm|1| in the optimization literature that focuses on solving multiple self-contained tasks simultaneously.  The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics. 
The key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate|2| the search on the other. 
The success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler|3| to more complex tasks. In practice an attempt is to intentionally|4| solve a more difficult task that may unintentionally solve several smaller problems.


== Methods ==
There are two common approaches for multi-task optimization: Bayesian optimization|5| and evolutionary computation.


=== Multi-task Bayesian|6| optimization ===
Multi-task Bayesian optimization is a modern model-based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms. The method builds a multi-task Gaussian
process model on the data originating from different searches progressing in tandem. The captured inter-task dependencies|7| are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces.


=== Evolutionary multi-tasking ===
Evolutionary multi-tasking has been explored as a means of exploiting the implicit parallelism of population-based search algorithms to simultaneously progress multiple distinct optimization tasks. By mapping all tasks to a unified search space, the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer. This is induced when solutions associated with different tasks crossover. Recently, modes of knowledge transfer that are different from direct solution crossover have been explored.


== Applications ==
Algorithms for multi-task optimization span a wide array of real-world applications. Recent studies highlight the potential for speed-ups in the optimization of engineering design parameters by conducting related designs jointly in a multi-task manner. In machine learning, the transfer of optimized|8| features across related data sets can enhance the efficiency of the training process as well as improve the generalization|9| capability of learned models. In addition, the concept of multi-tasking|10| has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning.Applications have also been reported in cloud computing, with future developments geared|11| towards cloud-based on-demand optimization services that can cater to multiple customers simultaneously.


== See also ==
Multi-objective optimization
Multi-task learning
Multicriteria classification
Multiple-criteria decision analysis


== References ==