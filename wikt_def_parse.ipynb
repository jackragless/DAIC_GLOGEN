{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import re, requests\n",
    "from utils import WordData, Definition, RelatedWord\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import zip_longest\n",
    "from copy import copy\n",
    "from string import digits\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original codebase\n",
    "\n",
    "\n",
    "PARTS_OF_SPEECH = [\n",
    "    \"noun\", \"verb\", \"adjective\", \"adverb\", \"determiner\",\n",
    "    \"article\", \"preposition\", \"conjunction\", \"proper noun\",\n",
    "    \"letter\", \"character\", \"phrase\", \"proverb\", \"idiom\",\n",
    "    \"symbol\", \"syllable\", \"numeral\", \"initialism\", \"interjection\",\n",
    "    \"definitions\", \"pronoun\",\n",
    "]\n",
    "\n",
    "RELATIONS = [\n",
    "    \"synonyms\", \"antonyms\", \"hypernyms\", \"hyponyms\",\n",
    "    \"meronyms\", \"holonyms\", \"troponyms\", \"related terms\",\n",
    "    \"coordinate terms\",\n",
    "]\n",
    "\n",
    "\n",
    "class WiktionaryParser(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"https://en.wiktionary.org/wiki/{}?printable=yes\"\n",
    "        self.soup = None\n",
    "        self.session = requests.Session()\n",
    "        self.session.mount(\n",
    "            \"http://\", requests.adapters.HTTPAdapter(max_retries=2))\n",
    "        self.session.mount(\n",
    "            \"https://\", requests.adapters.HTTPAdapter(max_retries=2))\n",
    "        self.language = 'english'\n",
    "        self.current_word = None\n",
    "        self.PARTS_OF_SPEECH = copy(PARTS_OF_SPEECH)\n",
    "        self.RELATIONS = copy(RELATIONS)\n",
    "        self.INCLUDED_ITEMS = self.RELATIONS + \\\n",
    "            self.PARTS_OF_SPEECH + ['etymology', 'pronunciation']\n",
    "\n",
    "    def include_part_of_speech(self, part_of_speech):\n",
    "        part_of_speech = part_of_speech.lower()\n",
    "        if part_of_speech not in self.PARTS_OF_SPEECH:\n",
    "            self.PARTS_OF_SPEECH.append(part_of_speech)\n",
    "            self.INCLUDED_ITEMS.append(part_of_speech)\n",
    "\n",
    "    def exclude_part_of_speech(self, part_of_speech):\n",
    "        part_of_speech = part_of_speech.lower()\n",
    "        self.PARTS_OF_SPEECH.remove(part_of_speech)\n",
    "        self.INCLUDED_ITEMS.remove(part_of_speech)\n",
    "\n",
    "    def include_relation(self, relation):\n",
    "        relation = relation.lower()\n",
    "        if relation not in self.RELATIONS:\n",
    "            self.RELATIONS.append(relation)\n",
    "            self.INCLUDED_ITEMS.append(relation)\n",
    "\n",
    "    def exclude_relation(self, relation):\n",
    "        relation = relation.lower()\n",
    "        self.RELATIONS.remove(relation)\n",
    "        self.INCLUDED_ITEMS.remove(relation)\n",
    "\n",
    "    def set_default_language(self, language=None):\n",
    "        if language is not None:\n",
    "            self.language = language.lower()\n",
    "\n",
    "    def get_default_language(self):\n",
    "        return self.language\n",
    "\n",
    "    def clean_html(self):\n",
    "        unwanted_classes = ['sister-wikipedia',\n",
    "                            'thumb', 'reference', 'cited-source']\n",
    "        for tag in self.soup.find_all(True, {'class': unwanted_classes}):\n",
    "            tag.extract()\n",
    "\n",
    "    def remove_digits(self, string):\n",
    "        return string.translate(str.maketrans('', '', digits)).strip()\n",
    "\n",
    "    def count_digits(self, string):\n",
    "        return len(list(filter(str.isdigit, string)))\n",
    "\n",
    "    def get_id_list(self, contents, content_type):\n",
    "        if content_type == 'etymologies':\n",
    "            checklist = ['etymology']\n",
    "        elif content_type == 'pronunciation':\n",
    "            checklist = ['pronunciation']\n",
    "        elif content_type == 'definitions':\n",
    "            checklist = self.PARTS_OF_SPEECH\n",
    "            if self.language == 'chinese':\n",
    "                checklist += self.current_word\n",
    "        elif content_type == 'related':\n",
    "            checklist = self.RELATIONS\n",
    "        else:\n",
    "            return None\n",
    "        id_list = []\n",
    "        if len(contents) == 0:\n",
    "            return [('1', x.title(), x) for x in checklist if self.soup.find('span', {'id': x.title()})]\n",
    "        for content_tag in contents:\n",
    "            content_index = content_tag.find_previous().text\n",
    "            text_to_check = self.remove_digits(\n",
    "                content_tag.text).strip().lower()\n",
    "            if text_to_check in checklist:\n",
    "                content_id = content_tag.parent['href'].replace('#', '')\n",
    "                id_list.append((content_index, content_id, text_to_check))\n",
    "        return id_list\n",
    "\n",
    "    def get_word_data(self, language):\n",
    "        contents = self.soup.find_all('span', {'class': 'toctext'})\n",
    "        word_contents = []\n",
    "        start_index = None\n",
    "        for content in contents:\n",
    "            if content.text.lower() == language:\n",
    "                start_index = content.find_previous().text + '.'\n",
    "        if len(contents) != 0 and not start_index:\n",
    "            return []\n",
    "        for content in contents:\n",
    "            index = content.find_previous().text\n",
    "            content_text = self.remove_digits(content.text.lower())\n",
    "            if index.startswith(start_index) and content_text in self.INCLUDED_ITEMS:\n",
    "                word_contents.append(content)\n",
    "        word_data = {\n",
    "            'examples': self.parse_examples(word_contents),\n",
    "            'definitions': self.parse_definitions(word_contents),\n",
    "            'etymologies': self.parse_etymologies(word_contents),\n",
    "            'related': self.parse_related_words(word_contents),\n",
    "            'pronunciations': self.parse_pronunciations(word_contents),\n",
    "        }\n",
    "        json_obj_list = self.map_to_object(word_data)\n",
    "        return json_obj_list\n",
    "\n",
    "    def parse_pronunciations(self, word_contents):\n",
    "        pronunciation_id_list = self.get_id_list(\n",
    "            word_contents, 'pronunciation')\n",
    "        pronunciation_list = []\n",
    "        audio_links = []\n",
    "        pronunciation_text = []\n",
    "        pronunciation_div_classes = ['mw-collapsible', 'vsSwitcher']\n",
    "        for pronunciation_index, pronunciation_id, _ in pronunciation_id_list:\n",
    "            span_tag = self.soup.find_all('span', {'id': pronunciation_id})[0]\n",
    "            list_tag = span_tag.parent\n",
    "            while list_tag.name != 'ul':\n",
    "                list_tag = list_tag.find_next_sibling()\n",
    "                if list_tag.name == 'p':\n",
    "                    pronunciation_text.append(list_tag.text)\n",
    "                    break\n",
    "                if list_tag.name == 'div' and any(_ in pronunciation_div_classes for _ in list_tag['class']):\n",
    "                    break\n",
    "            for super_tag in list_tag.find_all('sup'):\n",
    "                super_tag.clear()\n",
    "            for list_element in list_tag.find_all('li'):\n",
    "                for audio_tag in list_element.find_all('div', {'class': 'mediaContainer'}):\n",
    "                    audio_links.append(audio_tag.find('source')['src'])\n",
    "                    audio_tag.extract()\n",
    "                for nested_list_element in list_element.find_all('ul'):\n",
    "                    nested_list_element.extract()\n",
    "                if list_element.text and not list_element.find('table', {'class': 'audiotable'}):\n",
    "                    pronunciation_text.append(list_element.text.strip())\n",
    "            pronunciation_list.append(\n",
    "                (pronunciation_index, pronunciation_text, audio_links))\n",
    "        return pronunciation_list\n",
    "\n",
    "    def parse_definitions(self, word_contents):\n",
    "        definition_id_list = self.get_id_list(word_contents, 'definitions')\n",
    "        definition_list = []\n",
    "        definition_tag = None\n",
    "        for def_index, def_id, def_type in definition_id_list:\n",
    "            definition_text = []\n",
    "            span_tag = self.soup.find_all('span', {'id': def_id})[0]\n",
    "            table = span_tag.parent.find_next_sibling()\n",
    "            while table and table.name not in ['h3', 'h4', 'h5']:\n",
    "                definition_tag = table\n",
    "                table = table.find_next_sibling()\n",
    "                if definition_tag.name == 'p':\n",
    "                    definition_text.append(definition_tag.text.strip())\n",
    "                if definition_tag.name in ['ol', 'ul']:\n",
    "                    for element in definition_tag.find_all('li', recursive=False):\n",
    "                        if element.text:\n",
    "                            definition_text.append(element.text.strip())\n",
    "            if def_type == 'definitions':\n",
    "                def_type = ''\n",
    "            definition_list.append((def_index, definition_text, def_type))\n",
    "        return definition_list\n",
    "\n",
    "    def parse_examples(self, word_contents):\n",
    "\n",
    "        definition_id_list = self.get_id_list(word_contents, 'definitions')\n",
    "\n",
    "        html = BeautifulSoup(str(self.soup)[:[i.start() for i in re.finditer(\n",
    "            '<h2>', str(self.soup))][1]], 'html.parser')\n",
    "        # print(self.soup)\n",
    "\n",
    "        html = html.find('div', {'class': 'mw-parser-output'})\n",
    "        # print(html)\n",
    "\n",
    "        temp_pos = []\n",
    "        count = 0\n",
    "        for span in html.find_all('span', {'class': 'mw-headline'}):\n",
    "            if span.text.lower() in self.PARTS_OF_SPEECH:\n",
    "                temp_pos.append(span.text.lower())\n",
    "        final = {}\n",
    "        for i in range(len(temp_pos)):\n",
    "\n",
    "            temp = []\n",
    "            cur_ol = html.find_all('ol', recursive=False)[i]\n",
    "            for li in cur_ol.find_all('li', recursive=False):\n",
    "                if li.find('dl', recursive=False):\n",
    "                    example = li.find('dl', recursive=False).text\n",
    "                    if example.find(':') == -1:\n",
    "                        temp.append(li.find('dl', recursive=False).text)\n",
    "                    else:\n",
    "                        temp.append('null')\n",
    "#                 elif li.find('div', {'class': 'h-quotation'}):\n",
    "#                     temp.append(li.find('div', {'class': 'h-quotation'}).text)\n",
    "                elif li.find('li'):\n",
    "                    if li.find('li').find('dl'):\n",
    "                        temp.append( li.find('li').find('dl').text )\n",
    "                    else:\n",
    "                        temp.append('null')\n",
    "                else:\n",
    "                    temp.append('null')\n",
    "            if temp_pos[i] not in final.keys():\n",
    "                final[temp_pos[i]] = temp\n",
    "            else:\n",
    "                final[temp_pos[i]] += temp\n",
    "\n",
    "    # example_list = []\n",
    "        # for def_index, def_id, def_type in definition_id_list:\n",
    "        #     span_tag = self.soup.find_all('span', {'id': def_id})[0]\n",
    "        #     table = span_tag.parent\n",
    "        #     while table.name != 'ol':\n",
    "        #         table = table.find_next_sibling()\n",
    "        #     examples = []\n",
    "        #     # print(table)\n",
    "        #     while table and table.name == 'ol':\n",
    "\n",
    "        #         for element in table.find_all('dd'):\n",
    "        #             example_text = re.sub(r'\\([^)]*\\)', '', element.text.strip())\n",
    "        #             if example_text:\n",
    "        #                 examples.append(example_text)\n",
    "        #             element.clear()\n",
    "        #         example_list.append((def_index, examples, def_type))\n",
    "        #         for quot_list in table.find_all(['ul', 'ol']):\n",
    "        #             quot_list.clear()\n",
    "        #         table = table.find_next_sibling()\n",
    "        return final\n",
    "\n",
    "    def parse_etymologies(self, word_contents):\n",
    "        etymology_id_list = self.get_id_list(word_contents, 'etymologies')\n",
    "        etymology_list = []\n",
    "        etymology_tag = None\n",
    "        for etymology_index, etymology_id, _ in etymology_id_list:\n",
    "            etymology_text = ''\n",
    "            span_tag = self.soup.find_all('span', {'id': etymology_id})[0]\n",
    "            next_tag = span_tag.parent.find_next_sibling()\n",
    "            while next_tag.name not in ['h3', 'h4', 'div', 'h5']:\n",
    "                etymology_tag = next_tag\n",
    "                next_tag = next_tag.find_next_sibling()\n",
    "                if etymology_tag.name == 'p':\n",
    "                    etymology_text += etymology_tag.text\n",
    "                else:\n",
    "                    for list_tag in etymology_tag.find_all('li'):\n",
    "                        etymology_text += list_tag.text + '\\n'\n",
    "            etymology_list.append((etymology_index, etymology_text))\n",
    "        return etymology_list\n",
    "\n",
    "    def parse_related_words(self, word_contents):\n",
    "        relation_id_list = self.get_id_list(word_contents, 'related')\n",
    "        related_words_list = []\n",
    "        for related_index, related_id, relation_type in relation_id_list:\n",
    "            words = []\n",
    "            span_tag = self.soup.find_all('span', {'id': related_id})[0]\n",
    "            parent_tag = span_tag.parent\n",
    "            while not parent_tag.find_all('li'):\n",
    "                parent_tag = parent_tag.find_next_sibling()\n",
    "            for list_tag in parent_tag.find_all('li'):\n",
    "                words.append(list_tag.text)\n",
    "            related_words_list.append((related_index, words, relation_type))\n",
    "        return related_words_list\n",
    "\n",
    "    def map_to_object(self, word_data):\n",
    "        json_obj_list = []\n",
    "        if not word_data['etymologies']:\n",
    "            word_data['etymologies'] = [('', '')]\n",
    "        for (current_etymology, next_etymology) in zip_longest(word_data['etymologies'], word_data['etymologies'][1:], fillvalue=('999', '')):\n",
    "            data_obj = WordData()\n",
    "            data_obj.etymology = current_etymology[1]\n",
    "            for pronunciation_index, text, audio_links in word_data['pronunciations']:\n",
    "                if (self.count_digits(current_etymology[0]) == self.count_digits(pronunciation_index)) or (current_etymology[0] <= pronunciation_index < next_etymology[0]):\n",
    "                    data_obj.pronunciations = text\n",
    "                    data_obj.audio_links = audio_links\n",
    "            for definition_index, definition_text, definition_type in word_data['definitions']:\n",
    "                if current_etymology[0] <= definition_index < next_etymology[0]:\n",
    "                    def_obj = Definition()\n",
    "                    def_obj.text = definition_text\n",
    "                    def_obj.part_of_speech = definition_type\n",
    "\n",
    "                    # for examples in word_data['examples']:\n",
    "                    #     # if example_index.startswith(definition_index):\n",
    "                    #   def_obj.example_uses = examples\n",
    "                    for related_word_index, related_words, relation_type in word_data['related']:\n",
    "                        if related_word_index.startswith(definition_index):\n",
    "                            def_obj.related_words.append(\n",
    "                                RelatedWord(relation_type, related_words))\n",
    "                    data_obj.definition_list.append(def_obj)\n",
    "            json_obj_list.append(data_obj.to_json())\n",
    "        return json_obj_list, word_data['examples']\n",
    "\n",
    "    def fetch(self, word, language=None):\n",
    "        language = self.language if not language else language\n",
    "        response = self.session.get(self.url.format(word))\n",
    "        self.soup = BeautifulSoup(\n",
    "            response.text.replace('>\\n<', '><'), 'html.parser')\n",
    "        self.current_word = word\n",
    "        self.clean_html()\n",
    "        return self.get_word_data(language.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#additional code\n",
    "parser = WiktionaryParser()\n",
    "def wiktparser_restruct(defs):\n",
    "    dictionary = {}\n",
    "    for i in defs:\n",
    "        for j in i['definitions']:\n",
    "            if j['partOfSpeech'] not in dictionary.keys():\n",
    "                dictionary[j['partOfSpeech']] = []\n",
    "            for k in j['text'][1:]:\n",
    "                k = k.replace('etc.','etc').replace('e.g.','eg').replace('eg.','eg')\n",
    "                k = sent_tokenize(k)[0]\n",
    "                if not (k[0]=='('):\n",
    "                    dictionary[j['partOfSpeech']].append('(general) ' + k)\n",
    "                else:\n",
    "                    dictionary[j['partOfSpeech']].append(k)\n",
    "    return dictionary\n",
    "\n",
    "def define(kw):\n",
    "    wiki_object = parser.fetch(kw,'english')\n",
    "    examples = wiki_object[1]\n",
    "    restruct = wiktparser_restruct(wiki_object[0])\n",
    "    for key in restruct.keys():\n",
    "        temp = []\n",
    "        for i in range(len(restruct[key])):\n",
    "            try:\n",
    "                example = examples[key][i]\n",
    "                temp.append({'def':restruct[key][i].replace(example,''),'ex':example})\n",
    "            except:\n",
    "                example = 'null'\n",
    "                temp.append({'def':restruct[key][i],'ex':example})\n",
    "        restruct[key] = temp\n",
    "    return restruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noun': [{'def': '(chiefly Britain, usually a mass noun) Lodging in a dwelling or similar living quarters afforded to travellers in hotels or on cruise ships, or prisoners, etc.',\n",
       "   'ex': 'null'},\n",
       "  {'def': '(physical) Adaptation or adjustment.',\n",
       "   'ex': 'The accommodations at that hotel were quite luxurious.'},\n",
       "  {'def': '(personal) Adaptation or adjustment.',\n",
       "   'ex': 'It is true, the organization of the humane and animal Body, with accommodation to their several functions and offices, is certainly fitted with the most curious and exact Mechanism imaginable'},\n",
       "  {'def': '(countable, geology) The place where sediments can make, or have made, a sedimentation.',\n",
       "   'ex': 'null'},\n",
       "  {'def': \"(linguistics, sociolinguistics) Modifications to make one's way of speaking similar to others involved in a conversation or discourse; code-switching.\",\n",
       "   'ex': 'null'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define('accommodation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
