{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import pytorch\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('feature-extraction')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')\n",
    "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",       # Outside\n",
    "    \"B\",  # beginning\n",
    "    \"I\",  # inside\n",
    "\n",
    "reddit = praw.Reddit(client_id='my_client_id',\n",
    "                     client_secret='my_secret',\n",
    "                     user_agent='my user agent')\n",
    "\n",
    "\n",
    "def replies_of(top_level_comment, comment_list):\n",
    "    if len(top_level_comment.replies) == 0:\n",
    "        return\n",
    "    else:\n",
    "        for num, comment in enumerate(top_level_comment.replies):\n",
    "            try:\n",
    "                comment_list.append(str(comment.body))\n",
    "            except:\n",
    "                continue\n",
    "            replies_of(comment, comment_list)\n",
    "\n",
    "\n",
    "def main():\n",
    "    count = 0\n",
    "    master_dict = {'I-LOCX': [], 'I-ORGX': [], 'I-PERX': [], 'B-LOCX': [], 'B-ORGX': [], 'B-PERX': []}\n",
    "    word_temp = ''\n",
    "    current_tag = ''\n",
    "    old_tag = ''\n",
    "    print_dict = {}\n",
    "\n",
    "    list_of_subreddit = ['worldnews']\n",
    "    for j in list_of_subreddit:\n",
    "        # get 10 hot posts from the MachineLearning subreddit\n",
    "        top_posts = reddit.subreddit(j).top('week', limit=1)\n",
    "        comment_list = []\n",
    "        # save subreddit comments in dataframe\n",
    "        for submission in top_posts:\n",
    "            print('\\n\\n')\n",
    "            print(\"Title :\" , submission.title)\n",
    "            submission_comm = reddit.submission(id=submission.id)\n",
    "            comment_list.append(str(submission.title))\n",
    "\n",
    "            for count, top_level_comment in enumerate(submission_comm.comments):\n",
    "                try:\n",
    "                    replies_of(top_level_comment, comment_list)\n",
    "                except:\n",
    "                    continue\n",
    "#    print(comment_list)\n",
    "\n",
    "    # Bit of a hack to get the tokens with the special tokens\n",
    "    for sequence in comment_list:\n",
    "        if len(sequence) > 512:\n",
    "            continue\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "        inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "        outputs = model(inputs)[0]\n",
    "        predictions = tf.argmax(outputs, axis=2)\n",
    "        list_bert = [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]\n",
    "        for i in list_bert:\n",
    "            if i[1] in ['O', 'B-MISC', 'I-MISC']:\n",
    "                # print('in if')\n",
    "                if len(current_tag) > 0:\n",
    "                    without_space_word = word_temp.strip()\n",
    "                    if len(without_space_word) > 1:\n",
    "                        master_dict[current_tag + 'X'].append(without_space_word)\n",
    "                count = 0\n",
    "                word_temp = ''\n",
    "                current_tag = ''\n",
    "                continue\n",
    "            else:\n",
    "                current_tag = i[1]\n",
    "\n",
    "                if old_tag != current_tag and len(old_tag) > 0:\n",
    "                    without_space_word = word_temp.strip()\n",
    "                    if len(without_space_word) > 1:\n",
    "                        master_dict[old_tag + 'X'].append(without_space_word)\n",
    "                    count = 0\n",
    "                    word_temp = ''\n",
    "                    current_tag = ''\n",
    "\n",
    "                if i[0].startswith('##'):\n",
    "                    # print('in else if')\n",
    "                    word_temp += i[0][2:].upper()\n",
    "                elif i[1] in ['I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-PER']:\n",
    "                    # print('in end')\n",
    "                    word_temp += \" \" + i[0].upper()\n",
    "                    current_tag = i[1]\n",
    "                    count += 1\n",
    "                old_tag = current_tag\n",
    "\n",
    "    print(master_dict)\n",
    "    print_dict['Location'] = list(set(master_dict['I-LOCX'] + master_dict['B-LOCX']))\n",
    "    print_dict['Organisation'] = list(set(master_dict['I-ORGX'] + master_dict['B-ORGX']))\n",
    "    print_dict['Person Name'] = list(set(master_dict['I-PERX'] + master_dict['B-PERX']))\n",
    "    print('\\n\\n\\n')\n",
    "    print(print_dict)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
