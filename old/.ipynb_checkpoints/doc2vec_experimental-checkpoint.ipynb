{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decimal-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from wiktionary_parser.ipynb\n",
      "importing Jupyter notebook from preprocess_utils.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from bert_semanitic_similarity_experiments.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import import_ipynb\n",
    "import wiktionary_parser\n",
    "import preprocess_utils\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Phrases\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "\n",
    "import bert_semanitic_similarity_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores:\n",
    "# title to domain name (word2vec) = 75% (model1)\n",
    "# text to def sentence (doc2vec) = 6.25% (model2)\n",
    "# text to definition examples (bert semantic) = 78% (model3), 50%, 56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "induced-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gen(curtext):\n",
    "    curtext = preprocess_utils.clean_text(curtext,True,True,True,True,True)\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(curtext):\n",
    "        sent = sent[:-1]\n",
    "        temp = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            temp.append(word)\n",
    "        sents.append(temp)\n",
    "\n",
    "    tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(sents)]\n",
    "\n",
    "    model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "involved-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(title, domain, pos):\n",
    "    definition_sents =  [obj['def'] for obj in wiktionary_parser.define(domain)[pos]]\n",
    "    raw_domains = [nltk.word_tokenize(sent[:sent.find(')')].replace('(','').replace(')','').lower()) for sent in definition_sents]\n",
    "    domains = []\n",
    "    for arr in raw_domains:\n",
    "        temp = []\n",
    "        for word in arr:\n",
    "            if word not in stop_words and word!=',':\n",
    "                temp.append(word)\n",
    "        domains.append(temp)\n",
    "        \n",
    "    print(domains)\n",
    "\n",
    "    title_tokens = nltk.word_tokenize(title.lower())\n",
    "\n",
    "\n",
    "    FINAL = []\n",
    "    for dt in domains:\n",
    "        temp2 = []\n",
    "        for dtt in dt:\n",
    "            temp1 = []\n",
    "            for tt in title_tokens:\n",
    "                try:\n",
    "                    similarity = glove_vectors.similarity(dtt,tt)\n",
    "                except:\n",
    "                    similarity = 0\n",
    "                        \n",
    "                temp1.append(similarity)\n",
    "            temp2.append(np.mean(temp1))\n",
    "        FINAL.append(np.mean(temp2))\n",
    "    return FINAL, definition_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pending-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['general'], ['informal'], ['general'], ['computing'], ['boxing'], ['nautical'], ['obsolete'], ['general'], ['set', 'theory'], ['historical']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.10236509,\n",
       "  0.1184538,\n",
       "  0.10236509,\n",
       "  0.59832084,\n",
       "  0.0078193415,\n",
       "  0.022874743,\n",
       "  0.3741742,\n",
       "  0.10236509,\n",
       "  0.22859639,\n",
       "  0.07124958],\n",
       " ['(general) Any small rodent of the genus Mus.',\n",
       "  '(informal) A member of the many small rodent and marsupial species resembling such a rodent.',\n",
       "  '(general) A quiet or shy person.',\n",
       "  '(computing) (plural mice or, rarely, mouses) An input device that is moved over a pad or other flat surface to produce a corresponding movement of a pointer on a graphical display.',\n",
       "  '(boxing) A facial hematoma or black eye.',\n",
       "  '(nautical) A turn or lashing of spun yarn or small stuff, or a metallic clasp or fastening, uniting the point and shank of a hook to prevent its unhooking or straightening out.',\n",
       "  '(obsolete) A familiar term of endearment.Let the bloat King tempt you again to bed, / Pinch wanton on your cheek, call you his mouse',\n",
       "  '(general) A match used in firing guns or blasting.',\n",
       "  '(set theory) A small model of (a fragment of) Zermelo-Fraenkel set theory with desirable properties (depending on the context).',\n",
       "  \"(historical) A small cushion for a woman's hair.\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1('computers', 'mouse', 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "labeled-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(model, kw, pos):\n",
    "    term_to_define = [obj['def'] for obj in wiktionary_parser.define(kw)[pos]]\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for candidate in term_to_define:\n",
    "        candidate = preprocess_utils.clean_text(candidate,False,True,True,True,True)[:-1]\n",
    "        def_sent_candidate = []\n",
    "        for word in nltk.word_tokenize(candidate):\n",
    "            def_sent_candidate.append(word)\n",
    "\n",
    "#         print([' '.join(def_sent_candidate), mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(def_sent_candidate)], topn = 10000)])])\n",
    "\n",
    "        final.append(mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(def_sent_candidate)], topn = 10000)]))\n",
    "        \n",
    "    return final, term_to_define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brief-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(model, kw, pos):\n",
    "    \n",
    "    wikt_object = wiktionary_parser.define(kw)[pos]\n",
    "    examples = [i['ex'] for i in wikt_object]\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for ex in examples:\n",
    "        ex = preprocess_utils.clean_text(ex,True,True,True,True,True).replace('.',' ').strip()\n",
    "        ex_sent_candidate = []\n",
    "        for word in nltk.word_tokenize(ex):\n",
    "            ex_sent_candidate.append(word)\n",
    "\n",
    "        print([' '.join(ex_sent_candidate), mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(ex_sent_candidate)], topn = 10000)])])\n",
    "\n",
    "        final.append(mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(ex_sent_candidate)], topn = 10000)]))\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-works",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "curtext = preprocess_utils.clean_text(wikipedia.page('finance').content,True,True,True,True,True)\n",
    "model = model_gen(curtext)\n",
    "model3(model, 'index', 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_text = preprocess_utils.clean_text(wikipedia.page('Computers').content.lower())\n",
    "# model = model_gen(temp_text)\n",
    "# model_text_domain_plus_sent_predict(model, 'mouse', 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "primary-mumbai",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jackragless/projects/data/DAIC_GLOGEN/doc2vec_eval_data.pkl', 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = 0\n",
    "# count = 0\n",
    "# for subject in eval_data.keys():\n",
    "#     temp_text = preprocess_utils.clean_text(wikipedia.page(subject).content)\n",
    "#     model = model_gen(temp_text)\n",
    "#     for i in eval_data[subject]:\n",
    "#         count += 1\n",
    "#         temp_prob_arr, def_sents = model_text_domain_plus_sent_predict(model, i[0], i[1])\n",
    "#         temp_index = np.where(temp_prob_arr == np.max(temp_prob_arr))[0][0]\n",
    "#         guess = def_sents[temp_index]\n",
    "# #         print(i[0], '<--->', i[2], '<--->', guess)\n",
    "#         if guess.strip() == i[2].strip():\n",
    "#             score+=1\n",
    "# print(100*(score/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-schema",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = 0\n",
    "count = 0\n",
    "for subject in eval_data.keys():\n",
    "    for i in eval_data[subject]:\n",
    "        count += 1\n",
    "        temp_prob_arr, def_sents = model1(subject, i[0], i[1])\n",
    "        print(temp_prob_arr,def_sents)\n",
    "        temp_index = np.where(temp_prob_arr == np.max(temp_prob_arr))[0][0]\n",
    "        guess = def_sents[temp_index]\n",
    "        print(i[0], '<--->', i[2], '<--->', guess)\n",
    "        if guess.strip() == nltk.sent_tokenize(i[2].strip())[0]:\n",
    "            print(1,'\\n')\n",
    "            score+=1\n",
    "        else:\n",
    "            print(0,'\\n')\n",
    "print(100*(score/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = 0\n",
    "# count = 0\n",
    "# for subject in eval_data.keys():\n",
    "#     temp_text = preprocess_utils.clean_text(wikipedia.page(subject).content,True,True,True,True,True)\n",
    "#     model = model_gen(temp_text)\n",
    "#     for i in eval_data[subject]:\n",
    "#         count += 1\n",
    "#         temp_prob_arr, def_sents = model1(subject, i[0], i[1])\n",
    "#         examples_check = model3(model, i[0], i[1])\n",
    "#         if np.max(examples_check)>=0.98:\n",
    "#             temp_index = np.where(examples_check == np.max(examples_check))[0][0]\n",
    "#             guess = def_sents[temp_index]\n",
    "#         else:       \n",
    "#             temp_index = np.where(temp_prob_arr == np.max(temp_prob_arr))[0][0]\n",
    "#             guess = def_sents[temp_index]\n",
    "#         print(i[0], '<--->', i[2], '<--->', guess)\n",
    "#         if guess.strip() == nltk.sent_tokenize(i[2].strip())[0]:\n",
    "#             print(1,'\\n')\n",
    "#             score+=1\n",
    "#         else:\n",
    "#             print(0,'\\n')\n",
    "# print(100*(score/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-semiconductor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = 0\n",
    "count = 0\n",
    "for subject in eval_data.keys():\n",
    "    curtext = preprocess_utils.clean_text(wikipedia.page(subject).content,True,True,True,True,True)\n",
    "    for i in eval_data[subject]:\n",
    "        orig_definition_sents =   [obj['def'] for obj in wiktionary_parser.define(i[0])[i[1]]]\n",
    "        definition_sents = [preprocess_utils.clean_text(obj,False,True,True,True,True) for obj in orig_definition_sents]\n",
    "        temp_prob_arr = bert_semanitic_similarity_experiments.semantic_predict_4(curtext, definition_sents)\n",
    "        temp_index = np.where(temp_prob_arr == np.max(temp_prob_arr))[0][0]\n",
    "        guess = orig_definition_sents[temp_index]\n",
    "#         print(temp_prob_arr,orig_definition_sents)\n",
    "        print(i[0], '<--->', i[2], '<--->', guess)\n",
    "        if guess.strip() == nltk.sent_tokenize(i[2].strip())[0]:\n",
    "            print(1,'\\n')\n",
    "            score+=1\n",
    "        else:\n",
    "            print(0,'\\n')\n",
    "        count+=1\n",
    "print(100*(score/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adapted-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object <---> (object-oriented programming) An instantiation of a class or structure. <---> (object-oriented programming) An instantiation of a class or structure.\n",
      "1 \n",
      "\n",
      "class <---> (object-oriented programming, countable) A set of objects having the same behavior (but typically differing in state), or a template defining such a set. <---> (object-oriented programming, countable) A set of objects having the same behavior (but typically differing in state), or a template defining such a set.\n",
      "1 \n",
      "\n",
      "paradigm <---> An example serving as the model for such a pattern.  <---> (linguistics) A set of all forms which contain a common element, especially the set of all inflectional forms of a word or a particular grammatical category.\n",
      "0 \n",
      "\n",
      "loop <---> (programming) A programmed sequence of instructions that is repeated until or while a particular condition is satisfied. <---> (programming) A programmed sequence of instructions that is repeated until or while a particular condition is satisfied.\n",
      "1 \n",
      "\n",
      "short <---> (transitive, business) To sell something, especially securities, that one does not own at the moment for delivery at a later date in hopes of profiting from a decline in the price; to sell short. <---> (transitive, business) To sell something, especially securities, that one does not own at the moment for delivery at a later date in hopes of profiting from a decline in the price; to sell short.\n",
      "1 \n",
      "\n",
      "market <---> A formally organized, sometimes monopolistic, system of trading in specified goods or effects.  <---> (obsolete) The price for which a thing is sold in a market; hence, value; worth.\n",
      "0 \n",
      "\n",
      "index <---> (economics) A single number calculated from an array of prices or of quantities. <---> (economics) A single number calculated from an array of prices or of quantities.\n",
      "1 \n",
      "\n",
      "recession <---> A period of reduced economic activity <---> (general) A period of reduced economic activity\n",
      "0 \n",
      "\n",
      "beat <---> (music) A pulse on the beat level, the metric level at which pulses are heard as the basic unit. Thus a beat is the basic time unit of a piece. <---> (music) A pulse on the beat level, the metric level at which pulses are heard as the basic unit.\n",
      "1 \n",
      "\n",
      "harmony <---> (music) The academic study of chords. <---> (music) Two or more notes played simultaneously to produce a chord.\n",
      "0 \n",
      "\n",
      "forte <---> (music) Loud. Used as a dynamic directive in sheet music in its abbreviated form, \"f.\", to indicate raising the volume of the music. (Abbreviated in musical notation with an f, the Unicode character 1D191.) <---> (music) Loud.\n",
      "1 \n",
      "\n",
      "triangle <---> (music) A percussion instrument made by forming a metal rod into a triangular shape which is open at one angle. It is suspended from a string and hit with a metal bar to make a resonant sound. <---> (music) A percussion instrument made by forming a metal rod into a triangular shape which is open at one angle.\n",
      "1 \n",
      "\n",
      "mouse <---> (computing) (plural mice or, rarely, mouses) An input device that is moved over a pad or other flat surface to produce a corresponding movement of a pointer on a graphical display. <---> (computing) (plural mice or, rarely, mouses) An input device that is moved over a pad or other flat surface to produce a corresponding movement of a pointer on a graphical display.\n",
      "1 \n",
      "\n",
      "charge <---> (transitive) to load equipment with material required for its use, as a firearm with powder, a fire hose with water, a chemical reactor with raw materials <---> (transitive) to place a burden or load on or in\n",
      "the charging of children's memories [â€¦]  with rules1911, The Encyclopedia Britannica, entry on Moya:\n",
      "[A] huge torrent of boiling black mud, charged with blocks of rock and moving with enormous rapidity, rolled like an avalanche down the gorge.to ornament with or cause to bear\n",
      "to charge an architectural member with a moulding(heraldry) to assume as a bearingHe charges three roses.\n",
      "0 \n",
      "\n",
      "current <---> (electricity) the time rate of flow of electric charge. <---> (electricity) the time rate of flow of electric charge.\n",
      "1 \n",
      "\n",
      "monitor <---> (computing) A device similar to a television set used as to give a graphical display of the output from a computer. <---> (computing) A device similar to a television set used as to give a graphical display of the output from a computer.\n",
      "1 \n",
      "\n",
      "68.75\n",
      "time_taken = 578.695981502533\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "score = 0\n",
    "count = 0\n",
    "for subject in eval_data.keys():\n",
    "    curtext = preprocess_utils.clean_text(wikipedia.page(subject).content,True,True,True,True,True)\n",
    "    for i in eval_data[subject]:\n",
    "        orig_definition_sents =   [obj['def'] for obj in wiktionary_parser.define(i[0])[i[1]]]\n",
    "        definition_sents = [preprocess_utils.clean_text(obj,False,True,True,True,True) for obj in orig_definition_sents]\n",
    "        temp_prob_arr = bert_semanitic_similarity_experiments.semantic_predict_1(curtext, definition_sents)\n",
    "        temp_index = np.where(temp_prob_arr == np.max(temp_prob_arr))[0][0]\n",
    "        guess = orig_definition_sents[temp_index]\n",
    "#         print(temp_prob_arr,orig_definition_sents)\n",
    "        print(i[0], '<--->', i[2], '<--->', guess)\n",
    "        if guess.strip() == nltk.sent_tokenize(i[2].strip())[0]:\n",
    "            print(1,'\\n')\n",
    "            score+=1\n",
    "        else:\n",
    "            print(0,'\\n')\n",
    "        count+=1\n",
    "print(100*(score/count))\n",
    "print('time_taken = {}'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikt_def_gen.extract('forte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curtext = preprocess_utils.clean_text(wikipedia.page('Australia').content)\n",
    "# curtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-cassette",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compare doc to definition domain\n",
    "# for candidate in term_to_define:\n",
    "#     final_candidate = []\n",
    "#     final_candidate = candidate[candidate.find('(')+1:candidate.find(')')].replace(',','').lower().split()\n",
    "#     print(' '.join(final_candidate), '<--->', mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(final_candidate)], topn = len(tagged_data))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare doc to definition\n",
    "# for candidate in term_to_define:\n",
    "#     final_candidate = []\n",
    "#     final_candidate = candidate[candidate.find(')')+1:].replace(',','').lower().split()\n",
    "#     print(' '.join(final_candidate), '<--->', mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(final_candidate)], topn = len(tagged_data))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_text_domain_predict(model, kw, pos):\n",
    "#     term_to_define = wikt_def_gen.extract(kw)[pos]\n",
    "    \n",
    "#     final = []\n",
    "    \n",
    "#     for candidate in term_to_define:\n",
    "#         def_sent_candidate = []\n",
    "#         candidate = candidate[:-1].lower()\n",
    "#         candidate = candidate[candidate.find('(')+1:candidate.find(')')].replace(',','').lower().split()\n",
    "        \n",
    "#         final.append([candidate, mean([obj[1] for obj in model.docvecs.most_similar(positive=[model.infer_vector(candidate)], topn = 10000)])])\n",
    "        \n",
    "#     return final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
