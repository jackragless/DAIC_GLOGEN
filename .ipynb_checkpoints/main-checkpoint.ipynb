{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pyfiglet\n",
    "import os\n",
    "import wikipedia\n",
    "\n",
    "import import_ipynb\n",
    "import preprocess_utils\n",
    "import parse_utils\n",
    "import ai_kw_detect\n",
    "import wikt_def_parse\n",
    "import wikt_def_predict\n",
    "import kw_chink_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = ['Augmented reality-assisted surgery','Universal Scene Description','Junaio','USens','ARCore','Adjusted mutual information','Algorithmic information theory','Anti-information','Ascendency','Asymptotic equipartition property']\n",
    "# for page in pages:\n",
    "#     text_file = open(\"text_files/{}.txt\".format(page), \"w\")\n",
    "#     text_file.write(wikipedia.page(page,auto_suggest=False).content)\n",
    "#     text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_association = {\n",
    "    'CC':['conjunction'],\n",
    "    'CD':['numeral'],\n",
    "    'DT':['determiner'],\n",
    "    'EX':[],\n",
    "    'FW':[],\n",
    "    'IN':['preposition','conjunction'],\n",
    "    'JJ':['adjective'],\n",
    "    'JJR':['adjective'],\n",
    "    'JJS':['adjective'],\n",
    "    'LS':[],\n",
    "    'MD':['verb'],\n",
    "    'NN':['noun','proper noun'],\n",
    "    'NNS':['noun'], #PLURAL\n",
    "    'NNP':['noun', 'proper noun'],\n",
    "    'NNPS':['noun', 'proper noun'], #plural\n",
    "    'PDT':['determiner'],\n",
    "    'POS':[],\n",
    "    'PRP':['pronoun'],\n",
    "    'PRP$':['pronoun'],\n",
    "    'RB':['adverb'],\n",
    "    'RBR':['adverb'],\n",
    "    'RBS':['adverb'],\n",
    "    'RP':['preposition'], #unsure\n",
    "    'TO':[], #unsure\n",
    "    'UH':['interjection'],\n",
    "    'VB':['verb'],\n",
    "    'VBG':['verb'],\n",
    "    'VBD':['verb'],\n",
    "    'VBN':['verb'],\n",
    "    'VBP':['verb'],\n",
    "    'VBZ':['verb'],\n",
    "    'WDT':['determiner'],\n",
    "    'WP':['pronoun'],\n",
    "    'WRB':['adverb'],\n",
    "    #to deal with keyphrases\n",
    "    'noun':['noun', 'proper noun'],\n",
    "    'verb':['verb']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords(processed_text):\n",
    "    \n",
    "    candidate_phrases = []\n",
    "    pos = []\n",
    "    tok_sents = []\n",
    "    for sent in nltk.sent_tokenize(processed_text):\n",
    "        temp_tree = parse_utils.parseSent(sent)\n",
    "        if temp_tree:\n",
    "            tok_sents.append(sent[:-1])\n",
    "            candidate_phrases += parse_utils.getPhraseNodes(temp_tree,[])\n",
    "            pos.append( parse_utils.getWordNodes(temp_tree,[]) )\n",
    "        \n",
    "        \n",
    "    pred_kw = ai_kw_detect.predict(tok_sents)\n",
    "    \n",
    "    FINAL_KW = []\n",
    "    kw_only = []\n",
    "    for i in range(len(pred_kw)):\n",
    "        if pred_kw[i]:\n",
    "            for j in range(len(pred_kw[i])):\n",
    "                if pred_kw[i][j].find('.')==-1:\n",
    "                    if len(pred_kw[i][j].split()) == 1:\n",
    "                        for k in range(len(pos[i])):  \n",
    "                            if pred_kw[i][j] == pos[i][k][0] and pred_kw[i][j] not in kw_only:\n",
    "                                FINAL_KW.append([pos[i][k][0], pos_association[pos[i][k][1]]])\n",
    "                                kw_only.append(pos[i][k][0])\n",
    "                                break\n",
    "                    else:\n",
    "                        for m in range(len(candidate_phrases)):\n",
    "\n",
    "                            if pred_kw[i][j] == candidate_phrases[m][0] and pred_kw[i][j] not in kw_only:\n",
    "                                FINAL_KW.append([candidate_phrases[m][0],pos_association[candidate_phrases[m][1]]])\n",
    "                                kw_only.append(candidate_phrases[m][0])\n",
    "                                break\n",
    "                    \n",
    "        \n",
    "    return FINAL_KW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_definitions(title, clean_text, keyword_arr):\n",
    "    final = []\n",
    "    count = 0\n",
    "    for kw in keyword_arr:\n",
    "        count += 1\n",
    "        print(count,'/',len(keyword_arr), end='\\r')\n",
    "        \n",
    "        if kw[0].lower() not in [kw[0].lower() for kw in final]:\n",
    "            for pos in kw[1]:\n",
    "\n",
    "                temp_def = wikt_def_predict.driver(title.strip(), clean_text.strip(), kw[0].strip(), pos.strip(),0)\n",
    "                \n",
    "                if temp_def!='invalid-pos' and temp_def!='invalid-term':\n",
    "                    final.append([kw[0],temp_def.replace('invalid-pos','').replace('invalid-term','')])\n",
    "                    break\n",
    "                             \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_def_refs(orig_text, keywords_only):\n",
    "    indexes = []\n",
    "    index_sum = 0\n",
    "    for kw in keywords_only:\n",
    "        temp_index = orig_text[index_sum:].lower().find(kw.lower() + ' ') + len(kw)\n",
    "        index_sum += temp_index\n",
    "        indexes.append(index_sum)\n",
    "    \n",
    "    ref_text = orig_text\n",
    "    index_adjust = 0\n",
    "    for i in range(len(indexes)):\n",
    "        ref_text = ref_text[:indexes[i]+index_adjust] + '|{}|'.format(i+1) + ref_text[indexes[i]+index_adjust:]\n",
    "        index_adjust += len(str(i+1)) + 2\n",
    "        \n",
    "    return ref_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_doc(corpus_obj):\n",
    "    final_doc = '===GLOGEN GLOSSARY===\\n\\n'\n",
    "    \n",
    "    def_count = 0\n",
    "    for definition in corpus_obj['glossary']:\n",
    "        def_count += 1\n",
    "        temp_sent = definition[1]\n",
    "        final_doc += '|{}| '.format(def_count) + definition[0] + ' : ' + temp_sent + '\\n'\n",
    "        \n",
    "    final_doc += '\\n===DOCUMENT BODY===\\n\\n' + corpus_obj['text_w_ref']\n",
    "    \n",
    "    return final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(pyfiglet.figlet_format(\"DAIC GLOGEN\"),)\n",
    "# print('DESCRIPTION: GLOGEN automatically generates glossaries and prepends them to given .txt files. \\nENSURE: <filename>.txt == original text title.')\n",
    "# answer = ''\n",
    "# txt_address = ''\n",
    "# while True:\n",
    "#     answer = input('\\nType \"yes\" / \"no\" to starting GLOGEN:\\n>>>')\n",
    "#     if answer.lower().startswith(\"y\"):\n",
    "#         txt_address += input('Type address where .txt files are stored. If same address as main.py press ENTER.\\n>>>')\n",
    "#         break\n",
    "#     elif answer.lower().startswith(\"n\"):\n",
    "#         exit()\n",
    "#     else:\n",
    "#         print('INVALID INPUT --- TRY AGAIN.')\n",
    "#         continue\n",
    "# if txt_address == '':\n",
    "#     txt_address = os.getcwd()\n",
    "txt_address = '/home/jackragless/projects/github/DAIC_GLOGEN/text_files'\n",
    "if txt_address[-1] == '/':\n",
    "    txt_address = txt_address[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ascendency\n",
      "PREDICTING KEYWORDS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282b54d52f7f4c7e985deeacef7f7aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ae110c42f3472db787c8fc77f93d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING DEFINITIONS...\n",
      "11 / 11\n",
      " Anti-information\n",
      "PREDICTING KEYWORDS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d286145d4d604dbfaf1aacedfa88ea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e70977de8814f268007eee508c10b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING DEFINITIONS...\n",
      "2 / 2\r"
     ]
    }
   ],
   "source": [
    "user_input_data = []\n",
    "\n",
    "for filename in os.listdir(txt_address)[0:2]:\n",
    "    \n",
    "    if filename.endswith('.txt'):\n",
    "        print('\\n',filename[:-4])\n",
    "        orig_text = open(txt_address+'/'+filename).read()\n",
    "        \n",
    "        processed = preprocess_utils.clean_text(orig_text, False,False,True,False,False)\n",
    "        print('PREDICTING KEYWORDS...')\n",
    "        keywords_and_pos = generate_keywords(processed)\n",
    "        keywords_and_pos = kw_chink_utils.chink(keywords_and_pos)\n",
    "        \n",
    "        print('GENERATING DEFINITIONS...')\n",
    "        glossary = generate_definitions(filename[:-4], processed, keywords_and_pos)\n",
    "        keywords_only = [obj[0] for obj in glossary]\n",
    "    \n",
    "        user_input_data.append({\n",
    "             'title':filename[:-4], \n",
    "             'text_w_ref':add_def_refs(orig_text, keywords_only), \n",
    "             'keywords':keywords_and_pos, \n",
    "             'glossary': glossary\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(txt_address + '/GLOGEN') == False:\n",
    "    os.mkdir(txt_address + '/GLOGEN')\n",
    "for obj in user_input_data:\n",
    "    text_file = open(txt_address + \"/GLOGEN/{}.txt\".format(obj['title']), \"w\")\n",
    "    text_file.write( gen_final_doc(obj) )\n",
    "    text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
