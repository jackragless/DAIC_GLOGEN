{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "# import title_genre_predict\n",
    "import preprocess_utils\n",
    "import parse_utils\n",
    "import wiki_kw_retrieve\n",
    "import wikt_def_gen\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "import pyfiglet\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ____    _    ___ ____    ____ _     ___   ____ _____ _   _ \n",
      "|  _ \\  / \\  |_ _/ ___|  / ___| |   / _ \\ / ___| ____| \\ | |\n",
      "| | | |/ _ \\  | | |     | |  _| |  | | | | |  _|  _| |  \\| |\n",
      "| |_| / ___ \\ | | |___  | |_| | |__| |_| | |_| | |___| |\\  |\n",
      "|____/_/   \\_\\___\\____|  \\____|_____\\___/ \\____|_____|_| \\_|\n",
      "                                                            \n",
      "\n",
      "DESCRIPTION: GLOGEN automatically generates glossaries and prepends them to given .txt files. \n",
      "ENSURE: <filename>.txt == original text title.\n",
      "\n",
      "Type \"yes\" / \"no\" to starting GLOGEN:\n",
      ">>>yes\n",
      "Type address where .txt files are stored. If same address as main.py press ENTER.\n",
      ">>>/home/jackragless/projects/github/DAIC_GLOGEN/text_files\n"
     ]
    }
   ],
   "source": [
    "print(pyfiglet.figlet_format(\"DAIC GLOGEN\"),)\n",
    "print('DESCRIPTION: GLOGEN automatically generates glossaries and prepends them to given .txt files. \\nENSURE: <filename>.txt == original text title.')\n",
    "answer = ''\n",
    "txt_address = ''\n",
    "while True:\n",
    "    answer = input('\\nType \"yes\" / \"no\" to starting GLOGEN:\\n>>>')\n",
    "    if answer.lower().startswith(\"y\"):\n",
    "        txt_address += input('Type address where .txt files are stored. If same address as main.py press ENTER.\\n>>>')\n",
    "        break\n",
    "    elif answer.lower().startswith(\"n\"):\n",
    "        exit()\n",
    "    else:\n",
    "        print('INVALID INPUT --- TRY AGAIN.')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_data = []\n",
    "if txt_address == '':\n",
    "    txt_address = os.getcwd()\n",
    "    \n",
    "for filename in os.listdir(txt_address):\n",
    "    if filename.endswith('.txt'):\n",
    "        orig_text = open(txt_address+'/'+filename).read()\n",
    "        \n",
    "        processed = preprocess_utils.clean_text(orig_text)\n",
    "        \n",
    "        candidate_phrases = []\n",
    "        pos = []\n",
    "        for sent in nltk.sent_tokenize(processed):\n",
    "            temp_tree = parse_utils.parseSent(sent)\n",
    "            candidate_phrases.append( parse_utils.getPhraseNodes(temp_tree,[]) )\n",
    "            pos.append( parse_utils.getWordNodes(temp_tree,[]) )\n",
    "        \n",
    "        user_input_data.append({\n",
    "             'title':filename[:-4], \n",
    "#              'genre_preds':title_genre_predict.inference(filename[:-4]), \n",
    "             'keywords':np.nan, \n",
    "             'glossary':np.nan, \n",
    "             'raw_text':orig_text, \n",
    "             'pos':pos,\n",
    "             'candidate_phrases':candidate_phrases\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
