{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from preprocess_utils_ai.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ai_parse_utils.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/jackragless/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from chink_utils.ipynb\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jackragless/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/jackragless/miniconda3/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/home/jackragless/miniconda3/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import import_ipynb\n",
    "import preprocess_utils_ai\n",
    "import ai_parse_utils\n",
    "import chink_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jackragless/projects/data/DAIC_GLOGEN/wiki_orig_mined_dataframe.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generic-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(wiki_object):\n",
    "    \n",
    "    test_str = wiki_object['text']\n",
    "    if test_str.find('Version history') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('Version history', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "    elif test_str.find('See also') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('See also', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "    elif test_str.find('References') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('References', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "        \n",
    "        \n",
    "    clean_text = preprocess_utils_ai.clean_text(test_str,False,False,False,False,False)\n",
    "    \n",
    "    wiki_object['text'] = clean_text\n",
    "    \n",
    "    wiki_object['kw'].append(wiki_object['title'])\n",
    "    \n",
    "    return wiki_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welsh-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_add_kw(wiki_object):\n",
    "    kw_to_add = []\n",
    "    for sent in nltk.sent_tokenize(wiki_object['text']):\n",
    "        i = 0\n",
    "        while i < len(nltk.word_tokenize(sent)):\n",
    "            word = nltk.word_tokenize(sent)\n",
    "            if len(word[i])>3 and word[i].upper()==word[i] and word[i][0].isalpha():\n",
    "#                 kw_to_add.append(word[i]+'!!!')\n",
    "                kw_to_add.append(word[i])\n",
    "                i += 1\n",
    "            elif i>0 and word[i][0].isupper():\n",
    "                temp_kw_phrase = []\n",
    "                for j in range(i,len(word)):\n",
    "                    if word[j][0].isupper():\n",
    "                        temp_kw_phrase.append(word[j])\n",
    "                    else:\n",
    "                        break\n",
    "                if len(temp_kw_phrase)>=3:\n",
    "#                     temp_kw_phrase.append('!!!')\n",
    "                    kw_to_add.append(TreebankWordDetokenizer().detokenize(temp_kw_phrase))\n",
    "                    i += len(temp_kw_phrase)\n",
    "                else:\n",
    "                    i += 1 \n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "    return kw_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fiscal-algebra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING: 100 % <---> 100/100\r"
     ]
    }
   ],
   "source": [
    "#include only non-empty wiki objects\n",
    "#clean text in wiki_objects\n",
    "count = 0\n",
    "wiki_object = []\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i]['text'] and corpus[i]['kw'] and corpus[i]['title']:\n",
    "        wiki_object.append(clean_text(corpus[i]))\n",
    "    count += 1\n",
    "    print('CLEANING:', int(100*count/len(corpus)),'% <--->', str(count)+'/'+str(len(corpus)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in wiki_object:\n",
    "    trees = []\n",
    "    obj['kw'] += manual_add_kw(obj)\n",
    "    final_arr = []\n",
    "    for sent in nltk.sent_tokenize(obj['text']):\n",
    "        temp_tree = ai_parse_utils.parseSent(sent)\n",
    "        if temp_tree:\n",
    "            candidate_phrases = [obj[0] for obj in ai_parse_utils.getPhraseNodes(temp_tree,[])]\n",
    "            for i in range(len(candidate_phrases)):\n",
    "                if nltk.word_tokenize(candidate_phrases[i])[0].lower() in stop_words:\n",
    "                    candidate_phrases[i] = TreebankWordDetokenizer().detokenize(nltk.word_tokenize(candidate_phrases[i])[1:])\n",
    "        \n",
    "            found_sent = ''\n",
    "            lost_sent = sent\n",
    "            for candidate in candidate_phrases:\n",
    "#                 print(candidate)\n",
    "                index = lost_sent.find(candidate)\n",
    "                found_sent += lost_sent[:index]\n",
    "                lost_sent = lost_sent[index:]\n",
    "#                 print(found_sent)\n",
    "                lost_sent = lost_sent.replace(candidate,'XPHRASEX',1)\n",
    "            found_sent+=lost_sent\n",
    "#             print(found_sent)\n",
    "\n",
    "        temp_arr = []\n",
    "        count = 0\n",
    "        for word in nltk.word_tokenize(found_sent):\n",
    "            if word == 'XPHRASEX':\n",
    "                temp_arr.append(candidate_phrases[count])\n",
    "                count += 1\n",
    "                \n",
    "            else:\n",
    "                temp_arr.append(word)\n",
    "        final_arr.append(temp_arr)\n",
    "    obj['biogen_draft'] = final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in wiki_object:\n",
    "    i['kw'] = chink_utils.label_keyword_array(i['kw'], i['text'])\n",
    "    count += 1\n",
    "    print('CHINKING:', int(100*count/len(wiki_object)),'% <--->', str(count) + '/' + str(len(wiki_object)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "FINAL_OUTPUT = []\n",
    "for i in wiki_object:\n",
    "    temp_arr = []\n",
    "    for j in i['kw']:\n",
    "        if wiki_object[count]['kw'][j] == 'K' or wiki_object[count]['kw'][j] == 'P':\n",
    "            temp_arr.append(j)\n",
    "    temp = wiki_object[count]\n",
    "    temp['kw'] = temp_arr\n",
    "    FINAL_OUTPUT.append(temp)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_pool = []\n",
    "for page in FINAL_OUTPUT:\n",
    "    if page['kw']:\n",
    "        kw_pool += page['kw']\n",
    "kw_pool = list(set(kw_pool))\n",
    "kw_pool = list(np.sort(kw_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count = 0\n",
    "biogen = []\n",
    "for page in FINAL_OUTPUT:\n",
    "    for sent in page['biogen_draft']:\n",
    "        for word in sent:\n",
    "#             print(word)\n",
    "            if word in kw_pool:\n",
    "                biogen.append([sent_count,word,'T'])\n",
    "            else:\n",
    "                biogen.append([sent_count,word,'F'])\n",
    "        sent_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biogen_wconstparse_wcrossref.pkl', 'wb') as f:\n",
    "    pickle.dump(biogen, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
