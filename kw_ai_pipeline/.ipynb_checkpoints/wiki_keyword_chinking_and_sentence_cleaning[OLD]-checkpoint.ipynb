{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.4 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25h\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/jackragless/miniconda3/envs/book_title_classification/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/jackragless/miniconda3/envs/book_title_classification/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import spacy\n",
    "!python3 -m spacy download en\n",
    "import en_core_web_sm\n",
    "ner_model = en_core_web_sm.load()\n",
    "import pickle\n",
    "from names_dataset import NameDataset\n",
    "name_dataset = NameDataset()\n",
    "import os\n",
    "import re\n",
    "org_df = pd.read_csv('/home/jackragless/projects/local/DAIC_glossary_generator/big_data/companies_and_unis.csv')\n",
    "unigram = pd.read_csv('/home/jackragless/projects/local/DAIC_glossary_generator/big_data/unigram_freq.csv')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(wiki_object):\n",
    "    \n",
    "    test_str = wiki_object['text']\n",
    "    if test_str.find('Version history') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('Version history', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "    elif test_str.find('See also') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('See also', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "    elif test_str.find('References') != -1:\n",
    "        result = [i for i in range(len(test_str)) if test_str.startswith('References', i)] \n",
    "        test_str = test_str[:result[-1]]\n",
    "        \n",
    "        \n",
    "    clean_text = re.sub(\"[\\[\\(].*?[\\]\\)]\", \"\", test_str.replace('\\n',''))\n",
    "    final = ''\n",
    "    for i in clean_text:\n",
    "        if i.isalpha() or i.isdigit() or i=='-' or i==' ' or i=='.':\n",
    "            final += i\n",
    "            \n",
    "    \n",
    "    for i in range(len(final)-2):\n",
    "        if (final[i].isalpha() and final[i].islower() and final[i]!=' ') and (final[i+1] == '.') and (final[i+2].isalpha() and final[i+2].isupper()):\n",
    "            final = final[:i+1] + '. ' + final[i+2:]\n",
    "            \n",
    "    final = re.sub(' +', ' ', final).replace(' .','.').strip()\n",
    "    \n",
    "    wiki_object['text'] = final\n",
    "    \n",
    "    return wiki_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consec_cap_detect(word_string):\n",
    "    for i in range(0,len(word_string)-1):\n",
    "        if word_string[i].isupper() and word_string[i+1].isupper():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_detect(candidate_string):\n",
    "    candidate_string = candidate_string.split()\n",
    "    if len(candidate_string) == 2:\n",
    "        if name_dataset.search_first_name(candidate_string[0]) == True and name_dataset.search_last_name(candidate_string[1]) == True:\n",
    "            return True\n",
    "    elif len(candidate_string) == 3:\n",
    "        if name_dataset.search_first_name(candidate_string[0]) == True and len(candidate_string[1])>=2 and candidate_string[1][0].isalpha() and candidate_string[1][1] == '.':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_unigram = list(unigram[:10000]['word'])\n",
    "\n",
    "def common_word_detect(candidate_string): #could add stopwords\n",
    "    if candidate_string.lower() in common_unigram:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_name_detect(whole_text):\n",
    "    final = []\n",
    "    doc = ner_model(whole_text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            final.append(ent.text)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misc_filters(candidate_string):\n",
    "    if len(candidate_string) == 1:\n",
    "        return True\n",
    "    elif not any(c.isalpha() for c in candidate_string):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_keyword_array(kw_arr, text):\n",
    "    temp_dict = {}\n",
    "    for i in kw_arr:\n",
    "        if consec_cap_detect(i):\n",
    "            temp_dict[i] = 'P'\n",
    "        else:\n",
    "            temp_dict[i] = 'K'\n",
    "            \n",
    "    for j in temp_dict:\n",
    "        if (person_detect(j) or common_word_detect(j) or misc_filters(j)) and temp_dict[j]!='P':\n",
    "            temp_dict[j] = 'R'\n",
    "            \n",
    "    location_ners = location_name_detect(text)\n",
    "    for k in temp_dict:\n",
    "        if k in location_ners and temp_dict[j]!='P':\n",
    "            temp_dict[k] = 'R'\n",
    "        elif k.lower() in org_df:\n",
    "            temp_dict[k] = 'R'\n",
    "        elif text.find(k) == -1: #find if other reference content introduced via oversight\n",
    "            temp_dict[k] = 'R'\n",
    "    \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = text['text']\n",
    "    final = ''\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        sent = sent[:-1]\n",
    "        temp_sent = ''\n",
    "        for word in sent.split():\n",
    "            if word.lower() not in stop_words:\n",
    "                temp_sent += ' ' + word\n",
    "        final += ' ' + temp_sent.strip() + '.'\n",
    "    return final.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lumpers and splitters',\n",
       " 'text': 'Lumpers and splitters are opposing factions in any discipline that has to place individual examples into rigorously defined categories. The lumpersplitter problem occurs when there is the desire to create classifications and assign examples to them for example schools of literature biological taxa and so on. A lumper is an individual who takes a gestalt view of a definition and assigns examples broadly assuming that differences are not as important as signature similarities. A splitter is an individual who takes precise definitions and creates new categories to classify samples that differ in key ways. Origin of the terms The earliest known use of these terms was by Charles Darwin in a letter to Joseph Dalton Hooker in 1857 It is good to have hair-splitters lumpers. They were introduced more widely by George G. Simpson in his 1945 work The Principles of Classification and a Classification of Mammals. As he put it... splitters make very small units their critics say that if they can tell two animals apart they place them in different genera... and if they cannot tell them apart they place them in different species.... Lumpers make large units their critics say that if a carnivore is neither a dog nor a bear they call it a cat. A later use can be found in the title of a 1969 paper On lumpers and splitters... by the medical geneticist Victor McKusick. Reference to lumpers and splitters in the humanities appeared in a debate in 1975 between J. H. Hexter and Christopher Hill in the Times Literary Supplement. It followed from Hexters detailed review of Hills book Change and Continuity in Seventeenth Century England in which Hill developed Max Webers argument that the rise of capitalism was facilitated by Calvinist Puritanism. Hexter objected to Hills mining of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called lumping. According to him lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters by contrast emphasised differences and resisted simple schemes. While lumpers consistently tried to create coherent patterns splitters preferred incoherent complexity. Usage in various fields Biology The categorization and naming of a particular species should be regarded as a hypothesis about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand the hypothesis may be confirmed or refuted. Sometimes especially in the past when communication was more difficult taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two named species are agreed to be of the same species the older species name is almost always retained dropping the newer species name honoring a convention known as priority of nomenclature. This form of lumping is technically called synonymization. Dividing a taxon into multiple often new taxa is called splitting. Taxonomists are often referred to as lumpers or splitters by their colleagues depending on their personal approach to recognizing differences or commonalities between organisms. History In history lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities or groups which join fundamentally different examples. Splitting often leads to distinctions without difference ornate and fussy categories and failure to see underlying similarities. For example in the arts Romantic can refer specifically to a period of German poetry roughly from 17801810 but would exclude the later work of Goethe among other writers. In music it can mean every composer from Hummel through Rachmaninoff plus many that came after. Software modelling Software engineering often proceeds by building models. A lumper is keen to generalize and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example if error messages in two narrowly defined classes behave in the same way the classes can be easily combined. But if some messages in a broad class behave differently every object in the class must be examined before the class can be split. This illustrates the principle that splits can be lumped more easily than lumps can be split. Language classification There is no agreement among historical linguists about what amount of evidence is needed for two languages to be safely classified in the same language family. For this reason many language families have had lumpersplitter controversies including Altaic PamaNyungan Nilo-Saharan and most of the larger families of the Americas. At a completely different level the splitting of a mutually intelligible dialect continuum into different languages or lumping them into one is also an issue that continually comes up though the consensus in contemporary linguistics is that there is no completely objective way to settle the question. Splitters regard the comparative method as the only valid proof of kinship and consider genetic relatedness to be the question of interest. American linguists of recent decades tend to be splitters. Lumpers are more willing to admit techniques like mass lexical comparison or lexicostatistics and mass typological comparison and to tolerate the uncertainty of whether relationships found by these methods are the result of linguistic divergence or language convergence. Much long-range comparison work has been from Russian linguists belonging to the Moscow School of Comparative Linguistics most notably Vladislav Illich-Svitych and Sergei Starostin. In the United States Greenbergs and Ruhlens work has been met with little acceptance from linguists. Earlier American linguists like Morris Swadesh and Edward Sapir also pursued large-scale classifications like Sapirs 1929 scheme for the Americas accompanied by controversy similar to that today. Liturgical studies Paul F. Bradshaw suggests that the same principles of lumping and splitting apply to the study of early Christian liturgy. Lumpers who tend to predominate try to find a single line of texts from the apostolic age to the fourth century. Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value often there are hidden agendas in texts. The Hindu religion is essentially a lumpers concept sometimes also known as Smartism. Hindu splitters and individual adherents often identify themselves as adherents of a religion such as Shaivism Vaishnavism or Shaktism according to which deity they believe to be the supreme creator of the universe. Philosophy Physicist and philosophy writer Freeman Dyson has suggested that one can broadly if over-simplistically divide observers of the philosophical scene into splitters and lumpers - roughly corresponding to materialists and Platonists. Psychiatry In psychiatry thesplitters and the lumpers have fundamentally different approaches to psychiatric diagnosis and classification. First splitters emphasise the heterogeneity within the diagnostic categories and argue that this heterogeneity drives the splitting process. Lumpers on the other hand point to the similarities between the diagnostic categories and suggest that these similarities justify the creation of broader entities. Thus lumpers might see stress where splitters could identify worry grief or some sort of anxiety disorder.',\n",
       " 'kw': ['discipline',\n",
       "  'place individual examples into rigorously defined categories',\n",
       "  'literature',\n",
       "  'biological',\n",
       "  'taxa',\n",
       "  'gestalt',\n",
       "  'Charles Darwin',\n",
       "  'Joseph Dalton Hooker',\n",
       "  'George G. Simpson',\n",
       "  'carnivore',\n",
       "  'Victor McKusick',\n",
       "  'J. H. Hexter',\n",
       "  'Christopher Hill',\n",
       "  'Times Literary Supplement',\n",
       "  'Max Weber',\n",
       "  'Calvinist',\n",
       "  'Biological classification',\n",
       "  'evolutionary',\n",
       "  'organisms',\n",
       "  'Periodization',\n",
       "  'distinctions without difference',\n",
       "  'Romantic',\n",
       "  'German',\n",
       "  'Goethe',\n",
       "  'Hummel',\n",
       "  'Rachmaninoff',\n",
       "  'Software engineering',\n",
       "  'model-driven architecture',\n",
       "  'Language family',\n",
       "  'RSUH',\n",
       "  'Moscow School of Comparative Linguistics',\n",
       "  'Vladimir Dybo',\n",
       "  'Georgiy Starostin',\n",
       "  'historical linguists',\n",
       "  'language family',\n",
       "  'Altaic',\n",
       "  'Pama–Nyungan',\n",
       "  'Nilo-Saharan',\n",
       "  'families of the Americas',\n",
       "  'mutually intelligible',\n",
       "  'dialect continuum',\n",
       "  'comparative method',\n",
       "  'protolanguage',\n",
       "  'genetic',\n",
       "  'mass lexical comparison',\n",
       "  'lexicostatistics',\n",
       "  'linguistic divergence',\n",
       "  'language convergence',\n",
       "  'Moscow School of Comparative Linguistics',\n",
       "  'Vladislav Illich-Svitych',\n",
       "  'Sergei Starostin',\n",
       "  'Greenberg',\n",
       "  'Ruhlen',\n",
       "  'Morris Swadesh',\n",
       "  'Edward Sapir',\n",
       "  \"Sapir's 1929 scheme for the Americas\",\n",
       "  'Paul F. Bradshaw',\n",
       "  'liturgy',\n",
       "  'Smartism',\n",
       "  'Shaivism',\n",
       "  'Vaishnavism',\n",
       "  'Shaktism',\n",
       "  'citation needed',\n",
       "  'Freeman Dyson',\n",
       "  'materialists',\n",
       "  'Platonists',\n",
       "  'psychiatry',\n",
       "  '\"stress\"',\n",
       "  'worry',\n",
       "  'grief',\n",
       "  'anxiety disorder']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\r"
     ]
    }
   ],
   "source": [
    "input_df = []\n",
    "wiki_object = []\n",
    "directory = '/home/jackragless/projects/github/DAIC_glossary_generator/data/wiki_objects'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        with open(os.path.join(directory, filename), 'rb') as f:\n",
    "            input_df += pickle.load(f)\n",
    "count = 0;            \n",
    "for i in input_df:\n",
    "    if i['text'] is not None and i['kw'] is not None and i['title'] is not None:\n",
    "        wiki_object.append({'text':remove_stopwords(clean_text(i)),'kw':i['kw']})\n",
    "    count += 1\n",
    "    print(int(100*count/len(input_df)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/44853\r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in wiki_object: #change this back -----------------------------\n",
    "    replacement_dict = label_keyword_array(i['kw'], i['text'])\n",
    "    i['kw'] = replacement_dict\n",
    "    count += 1\n",
    "    print(str(count) + '/' + str(len(wiki_object)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "FINAL = []\n",
    "for i in wiki_object:\n",
    "    temp_arr = []\n",
    "    for j in i['kw']:\n",
    "        if wiki_object[count]['kw'][j] == 'K' or wiki_object[count]['kw'][j] == 'P':\n",
    "            temp_arr.append(j)\n",
    "    temp = wiki_object[count]\n",
    "    temp['kw'] = temp_arr\n",
    "    FINAL.append(temp)\n",
    "    count += 1\n",
    "    print(count, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jackragless/projects/github/DAIC_glossary_generator/data/wiki_master_dataframe_xstopwordsx_removed.pkl', 'wb') as f:\n",
    "    pickle.dump(FINAL, f)\n",
    "with open('/home/jackragless/projects/github/DAIC_glossary_generator/data/wiki_master_dataframe_xstopwordsx_labelled.pkl', 'wb') as f:\n",
    "    pickle.dump(wiki_object, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
