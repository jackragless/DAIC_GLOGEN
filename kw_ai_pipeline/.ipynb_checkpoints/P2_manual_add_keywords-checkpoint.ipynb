{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automated-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ai_parse_utils\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "import pandas as pd\n",
    "unigram = pd.read_csv('/home/jackragless/projects/data/DAIC_GLOGEN/unigram_freq.csv')\n",
    "common_unigram = list(unigram[:10000]['word'])\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "yellow-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_add_kw(wiki_object):\n",
    "    kw_to_add = []\n",
    "    for sent in nltk.sent_tokenize(wiki_object['text']):\n",
    "        i = 0\n",
    "        while i < len(nltk.word_tokenize(sent)):\n",
    "            word = nltk.word_tokenize(sent)\n",
    "            if len(word[i])>3 and word[i].upper()==word[i] and word[i][0].isalpha():\n",
    "                kw_to_add.append(word[i])\n",
    "                i += 1\n",
    "            elif i>0 and word[i][0].isupper():\n",
    "                temp_kw_phrase = []\n",
    "                for j in range(i,len(word)):\n",
    "                    if word[j][0].isupper():\n",
    "                        temp_kw_phrase.append(word[j])\n",
    "                    else:\n",
    "                        break\n",
    "                if len(temp_kw_phrase)>=3:\n",
    "                    kw_to_add.append(TreebankWordDetokenizer().detokenize(temp_kw_phrase))\n",
    "                    i += len(temp_kw_phrase)\n",
    "                else:\n",
    "                    i += 1 \n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "    return kw_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "matched-production",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def driver(corpus):\n",
    "\n",
    "    tb_corpus = []\n",
    "    count = 0\n",
    "    length = len(corpus)\n",
    "    for page in corpus:\n",
    "        count += 1\n",
    "        print(str(int(100*count/length)) + '%','<--->',count,'/',length,end='\\r')\n",
    "        if page['text'] and page['kw'] and page['title']:\n",
    "            page['kw'] += manual_add_kw(page)\n",
    "            temp_doc = ''\n",
    "            for sent in nltk.sent_tokenize(page['text']):\n",
    "                temp_tree = ai_parse_utils.parseSent(sent)\n",
    "                if temp_tree:\n",
    "                    candidate_phrases = [page[0] for page in ai_parse_utils.getPhraseNodes(temp_tree,[])]\n",
    "                    for i in range(len(candidate_phrases)):\n",
    "                        if nltk.word_tokenize(candidate_phrases[i])[0].lower() in stop_words:\n",
    "                            candidate_phrases[i] = TreebankWordDetokenizer().detokenize(nltk.word_tokenize(candidate_phrases[i])[1:])\n",
    "                    temp_sent = sent\n",
    "                    for candidate in candidate_phrases:\n",
    "                        temp_sent = temp_sent.replace(candidate,candidate.replace(' ','ZZZ'))\n",
    "                temp_doc += ' ' + temp_sent\n",
    "            tb_corpus.append(tb(temp_doc))\n",
    "        else:\n",
    "            corpus.remove(page)\n",
    "            \n",
    "            \n",
    "    for i, blob in enumerate(tb_corpus):\n",
    "        scores = {word: tfidf(word, blob, tb_corpus) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        temp = []\n",
    "        for word, score in sorted_words[:int(0.05*len(blob.words))]:\n",
    "            if word.lower() not in common_unigram and word.lower() not in stop_words:\n",
    "                temp.append(word.replace('ZZZ',' '))\n",
    "        if corpus[i]['kw']:\n",
    "            corpus[i]['kw'] += temp\n",
    "        else:\n",
    "            corpus[i]['kw'] = temp\n",
    "        corpus[i]['kw'] = list(set(corpus[i]['kw']))\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacterial-meaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 / 86\r"
     ]
    }
   ],
   "source": [
    "# with open('/home/jackragless/projects/data/DAIC_GLOGEN/wiki_orig_mined_dataframe.pkl', 'rb') as f:\n",
    "#     corpus = pickle.load(f)[0:100]\n",
    "# final = driver(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
